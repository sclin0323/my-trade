{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正規表達法斷句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "article = '''\n",
    "\n",
    "【綜合報導】違建真害命！違建大火連續燒，近2周內，光雙北市至少有10人因密集隔間違建火警喪命，新北市府日前展開大動作，強拆一棟2層樓加蓋成6層樓、隔成158間房分租的離譜違建，強調明起全面執行頂加分租套房拆除。台北市長柯文哲昨也表示，考慮拿掉前市長陳水扁下的「違建特赦令」，涉及公安的大型違建，不論是既存違建或是新違建，都要依法處置。目前雙北共有29萬戶列管違建，若不改善，恐將面臨拆除命運。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n【綜合報導】違建真害命',\n",
       " '違建大火連續燒',\n",
       " '近2周內',\n",
       " '光雙北市至少有10人因密集隔間違建火警喪命',\n",
       " '新北市府日前展開大動作',\n",
       " '強拆一棟2層樓加蓋成6層樓、隔成158間房分租的離譜違建',\n",
       " '強調明起全面執行頂加分租套房拆除',\n",
       " '台北市長柯文哲昨也表示',\n",
       " '考慮拿掉前市長陳水扁下的「違建特赦令」',\n",
       " '涉及公安的大型違建',\n",
       " '不論是既存違建或是新違建',\n",
       " '都要依法處置',\n",
       " '目前雙北共有29萬戶列管違建',\n",
       " '若不改善',\n",
       " '恐將面臨拆除命運',\n",
       " '\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('！|，|。',article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in c:\\programdata\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "! pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.176 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大\n",
      "巨蛋\n",
      "案對\n",
      "市府\n",
      "同仁\n",
      "下\n",
      "封口令\n",
      "?\n",
      " \n",
      "柯\n",
      "P\n",
      "否認\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "for ele in jieba.cut('大巨蛋案對市府同仁下封口令? 柯P否認'):\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大/巨蛋/案對/市府/同仁/下/封口令/?/ /柯/P/否認'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(jieba.cut('大巨蛋案對市府同仁下封口令? 柯P否認'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大/巨蛋/案/對/市府/同仁/下/封口/封口令/口令////柯/P/否/認'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(jieba.cut('大巨蛋案對市府同仁下封口令? 柯P否認', cut_all=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大/巨蛋/案對/市府/同仁/下/封口令/?/ /柯/P/否認'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(jieba.cut('大巨蛋案對市府同仁下封口令? 柯P否認'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取使用者字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict('userdict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大巨蛋/案對/市府/同仁/下/封口令/?/ /柯P/否認'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(jieba.cut('大巨蛋案對市府同仁下封口令? 柯P否認'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抓出詞性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大巨蛋 n\n",
      "案 ng\n",
      "對 p\n",
      "市府 n\n",
      "同仁 nr\n",
      "下 f\n",
      "封口令 n\n",
      "? x\n",
      "  x\n",
      "柯P n\n",
      "否認 v\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut('大巨蛋案對市府同仁下封口令? 柯P否認')\n",
    "for w in words:\n",
    "    print(w.word, w.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('大巨蛋', 0, 3)\n",
      "('案對', 3, 5)\n",
      "('市府', 5, 7)\n",
      "('同仁', 7, 9)\n",
      "('下', 9, 10)\n",
      "('封口令', 10, 13)\n",
      "('?', 13, 14)\n",
      "(' ', 14, 15)\n",
      "('柯P', 15, 17)\n",
      "('否認', 17, 19)\n"
     ]
    }
   ],
   "source": [
    "sentence = '大巨蛋案對市府同仁下封口令? 柯P否認'\n",
    "for tw in jieba.tokenize(sentence):\n",
    "    print(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['封口令']\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "tags = jieba.analyse.extract_tags(sentence, 1)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['同仁']\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "tags = jieba.analyse.extract_tags(sentence, 1, allowPOS=['nr'])\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 擴充字典\n",
    "- https://www.moedict.tw/\n",
    "\n",
    "- https://zh.wikipedia.org/wiki/%E5%94%90%E7%B4%8D%E5%BE%B7%C2%B7%E5%B7%9D%E6%99%AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用新聞關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('http://news.ltn.com.tw/news/business/breakingnews/2272811')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [ele.text for ele in soup.select('.keyword a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('userdict.txt', 'a', encoding='utf-8') as f:\n",
    "    f.write('\\n')\n",
    "    for keyword in keywords:\n",
    "        f.write(keyword + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得保險詞彙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://www.ib.gov.tw/ch/home.jsp?id=59&parentpath=0,6&mcustomize='\n",
    "payload = {\n",
    "'id':'59',\n",
    "'contentid':'59',\n",
    "'parentpath':'0,6',\n",
    "'mcustomize':'bilingual_list.jsp',\n",
    "'ckeyword':'請輸入中文關鍵字',\n",
    "'ekeyword':'請輸入英文關鍵字',\n",
    "'page':'3'\n",
    "}\n",
    "\n",
    "res = requests.post(url, data = payload)\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "keywords = [ele.text for ele in soup.select('.bich_name_con')]\n",
    "with open('userdict.txt', 'a', encoding='utf-8') as f:\n",
    "    f.write('\\n')\n",
    "    for keyword in keywords:\n",
    "        f.write(keyword + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將PDF 轉成文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer3k in c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: ply>=3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer3k)\n",
      "Requirement already satisfied: pytest>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer3k)\n",
      "Requirement already satisfied: py>=1.4.29 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k)\n"
     ]
    }
   ],
   "source": [
    "! pip install pdfminer3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = requests.get('https://www.fubon.com/life/public_info/public_info_04/5-12.pdf')\n",
    "with open('fubon.pdf', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## from pdfminer.pdfparser import PDFParser, PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "\n",
    "s = ''\n",
    "fp = open('fubon.pdf', 'rb')\n",
    "parser = PDFParser(fp)\n",
    "doc = PDFDocument()\n",
    "parser.set_document(doc)\n",
    "doc.set_parser(parser)\n",
    "doc.initialize('')\n",
    "rsrcmgr = PDFResourceManager()\n",
    "laparams = LAParams()\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "# Process each page contained in the document.\n",
    "for page in doc.get_pages():\n",
    "    interpreter.process_page(page)\n",
    "    layout = device.get_result()\n",
    "    for lt_obj in layout:\n",
    "        if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "            #print(lt_obj.get_text())\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將文字轉成 Word 檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python-docx-0.8.6.tar.gz (5.3MB)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Running setup.py bdist_wheel for python-docx: started\n",
      "  Running setup.py bdist_wheel for python-docx: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\User\\AppData\\Local\\pip\\Cache\\wheels\\cc\\74\\10\\42b00d7d6a64cf21f194bfef9b94150009ada880f13c5b2ad3\n",
      "Successfully built python-docx\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.6\n"
     ]
    }
   ],
   "source": [
    "! pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\docx\\styles\\styles.py:54: UserWarning: style lookup by style_id is deprecated. Use style name as key instead.\n",
      "  warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "document = Document()\n",
    "\n",
    "document.add_heading('Document Title', 0)\n",
    "\n",
    "p = document.add_paragraph('A plain paragraph having some ')\n",
    "p.add_run('bold').bold = True\n",
    "p.add_run(' and some ')\n",
    "p.add_run('italic.').italic = True\n",
    "\n",
    "document.add_heading('Heading, level 1', level=1)\n",
    "document.add_paragraph('Intense quote', style='IntenseQuote')\n",
    "\n",
    "document.add_paragraph(\n",
    "    'first item in unordered list', style='ListBullet'\n",
    ")\n",
    "document.add_paragraph(\n",
    "    'first item in ordered list', style='ListNumber'\n",
    ")\n",
    "\n",
    "\n",
    "document.add_page_break()\n",
    "\n",
    "document.save('demo.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將PDF轉 WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.layout:Too many boxes (101) to group, skipping.\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "document = Document()\n",
    "\n",
    "\n",
    "fp = open('fubon.pdf', 'rb')\n",
    "parser = PDFParser(fp)\n",
    "doc = PDFDocument()\n",
    "parser.set_document(doc)\n",
    "doc.set_parser(parser)\n",
    "doc.initialize('')\n",
    "rsrcmgr = PDFResourceManager()\n",
    "laparams = LAParams()\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "# Process each page contained in the document.\n",
    "for page in doc.get_pages():\n",
    "    interpreter.process_page(page)\n",
    "    layout = device.get_result()\n",
    "    for lt_obj in layout:\n",
    "        if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "            document.add_paragraph(lt_obj.get_text())\n",
    "    document.add_page_break()\n",
    "document.save('fubon.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = '那我們酸民婉君也可以報名嗎'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'那我'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我們'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那我\n",
      "我們\n",
      "們酸\n",
      "酸民\n",
      "民婉\n",
      "婉君\n",
      "君也\n",
      "也可\n",
      "可以\n",
      "以報\n",
      "報名\n",
      "名嗎\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(sentence) - 2 + 1):\n",
    "    print(sentence[i:i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那我們\n",
      "我們酸\n",
      "們酸民\n",
      "酸民婉\n",
      "民婉君\n",
      "婉君也\n",
      "君也可\n",
      "也可以\n",
      "可以報\n",
      "以報名\n",
      "報名嗎\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(sentence) - 3 + 1):\n",
    "    print(sentence[i:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(sentence, n = 2):\n",
    "    words = []\n",
    "    for i in range(0, len(sentence) - n + 1):\n",
    "        words.append(sentence[i:i+n])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['那我們酸',\n",
       " '我們酸民',\n",
       " '們酸民婉',\n",
       " '酸民婉君',\n",
       " '民婉君也',\n",
       " '婉君也可',\n",
       " '君也可以',\n",
       " '也可以報',\n",
       " '可以報名',\n",
       " '以報名嗎']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram(sentence, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = '''\n",
    "全台最後一場週年慶今日由新光三越台北站前店壓軸登場，化妝品推出20萬組明星特惠組、保暖外套全面5折起，吸引上千民眾排隊搶購，館內一度塞車7分鐘。周末又有一波強烈冷氣團來襲，百貨業者看好禦寒商品、抗空污家電持續成長，預估首日來客數可達7萬人，業績上看4.5億元，成長1%。\n",
    " \n",
    "台北站前店店長曹龍生表示，今年化妝品表現最好，各品牌口紅都熱賣，年輕美眉也人手一支，為彩妝引進許多新客，帶動業績大幅成長。另外近來全台深受空污影響，讓空氣清淨機熱賣，如LG空氣清淨機大缺貨，家電新機種多優先在百貨開賣，搭配滿千送百更划算。\n",
    " \n",
    "冷氣團一波接著一波，毛料外套、羽絨被也相當受歡迎，如羽毛工房限量100件，開店不久即搶光。新光三越台北站前店推出話題大衣全面5折起，共計200櫃全面力拚比市場再低1折，500多件保暖外套平均1000~1500元；保暖冬被1折起。營業副理陳英玒看好在空污議題、低溫助攻下，家電業績可成長30%。\n",
    " \n",
    "民眾陳小姐表示今天以靴款為採買重點；從事行政的王小姐則主攻保養品、彩妝、香水，偏好在同一櫃買齊，折扣累積更划算，預算無上限；從事補教業的蒲先生同樣以購買保養品為主，保濕、眼霜、精華液等，大約花費1萬元。\n",
    " \n",
    "新光三越台北站前店今起至24日為期18天，首波強勢祭出超值禮券回饋，20日前化妝品滿2,000送200、全館滿5,000送500、大家電/法雅客滿萬送500、13日前加碼貴賓卡獨享大家電滿萬送仟。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 13\n",
      "家電 5\n",
      "。\n",
      " 5\n",
      "台北 4\n",
      "北站 4\n",
      "站前 4\n",
      "前店 4\n",
      "20 4\n",
      "成長 4\n",
      "\n",
      "  4\n",
      " \n",
      " 4\n",
      "50 4\n",
      "新光 3\n",
      "光三 3\n",
      "三越 3\n",
      "越台 3\n",
      "化妝 3\n",
      "妝品 3\n",
      "保暖 3\n",
      "外套 3\n",
      "全面 3\n",
      "折起 3\n",
      "一波 3\n",
      "空污 3\n",
      "業績 3\n",
      "賣， 3\n",
      "0、 3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words = ngram(news, n = 2)\n",
    "words_dic = Counter(words)\n",
    "for k , v in words_dic.most_common(30):\n",
    "    if v >= 3:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台北站 4\n",
      "北站前 4\n",
      "站前店 4\n",
      "。\n",
      "  4\n",
      "\n",
      " \n",
      " 4\n",
      "500 4\n",
      "新光三 3\n",
      "光三越 3\n",
      "三越台 3\n",
      "越台北 3\n",
      "化妝品 3\n",
      "000 3\n",
      "00、 3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words = ngram(news, n = 3)\n",
    "words_dic = Counter(words)\n",
    "for k , v in words_dic.most_common(30):\n",
    "    if v >= 3:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台北站前 4\n",
      "北站前店 4\n",
      "。\n",
      " \n",
      " 4\n",
      "新光三越 3\n",
      "光三越台 3\n",
      "三越台北 3\n",
      "越台北站 3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words = ngram(news, n = 4)\n",
    "words_dic = Counter(words)\n",
    "for k , v in words_dic.most_common(30):\n",
    "    if v >= 3:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "全台最後一場週年慶今日由新光三越台北站前店壓軸登場\n",
      "化妝品推出20萬組明星特惠組\n",
      "保暖外套全面5折起\n",
      "吸引上千民眾排隊搶購\n",
      "館內一度塞車7分鐘\n",
      "周末又有一波強烈冷氣團來襲\n",
      "百貨業者看好禦寒商品\n",
      "抗空污家電持續成長\n",
      "預估首日來客數可達7萬人\n",
      "業績上看4.5億元\n",
      "成長1%\n",
      "\n",
      " \n",
      "台北站前店店長曹龍生表示\n",
      "今年化妝品表現最好\n",
      "各品牌口紅都熱賣\n",
      "年輕美眉也人手一支\n",
      "為彩妝引進許多新客\n",
      "帶動業績大幅成長\n",
      "另外近來全台深受空污影響\n",
      "讓空氣清淨機熱賣\n",
      "如LG空氣清淨機大缺貨\n",
      "家電新機種多優先在百貨開賣\n",
      "搭配滿千送百更划算\n",
      "\n",
      " \n",
      "冷氣團一波接著一波\n",
      "毛料外套\n",
      "羽絨被也相當受歡迎\n",
      "如羽毛工房限量100件\n",
      "開店不久即搶光\n",
      "新光三越台北站前店推出話題大衣全面5折起\n",
      "共計200櫃全面力拚比市場再低1折\n",
      "500多件保暖外套平均1000~1500元\n",
      "保暖冬被1折起\n",
      "營業副理陳英玒看好在空污議題\n",
      "低溫助攻下\n",
      "家電業績可成長30%\n",
      "\n",
      " \n",
      "民眾陳小姐表示今天以靴款為採買重點\n",
      "從事行政的王小姐則主攻保養品\n",
      "彩妝\n",
      "香水\n",
      "偏好在同一櫃買齊\n",
      "折扣累積更划算\n",
      "預算無上限\n",
      "從事補教業的蒲先生同樣以購買保養品為主\n",
      "保濕\n",
      "眼霜\n",
      "精華液等\n",
      "大約花費1萬元\n",
      "\n",
      " \n",
      "新光三越台北站前店今起至24日為期18天\n",
      "首波強勢祭出超值禮券回饋\n",
      "20日前化妝品滿2,000送200\n",
      "全館滿5,000送500\n",
      "大家電\n",
      "法雅客滿萬送500\n",
      "13日前加碼貴賓卡獨享大家電滿萬送仟\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for ele in re.split('，|。|、|；|/', news):\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeKey(sentence, keywords):\n",
    "    for keyword in keywords:\n",
    "        sentence = sentence.replace(keyword, '')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeKey2(sentence, keywords):\n",
    "    for keyword in keywords:\n",
    "        sentence = ''.join(sentence.split(keyword))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n全台最後一場週年慶今日由台北站前店壓軸登場，推出20萬組明星特惠組、保暖外套全面5折起，吸引上千民眾排隊搶購，館內一度塞車7分鐘。周末又有一波強烈冷氣團來襲，百貨業者看好禦寒商品、抗空污家電持續成長，預估首日來客數可達7萬人，業績上看4.5億元，成長1%。\\n \\n台北站前店店長曹龍生表示，今年表現最好，各品牌口紅都熱賣，年輕美眉也人手一支，為彩妝引進許多新客，帶動業績大幅成長。另外近來全台深受空污影響，讓空氣清淨機熱賣，如LG空氣清淨機大缺貨，家電新機種多優先在百貨開賣，搭配滿千送百更划算。\\n \\n冷氣團一波接著一波，毛料外套、羽絨被也相當受歡迎，如羽毛工房限量100件，開店不久即搶光。台北站前店推出話題大衣全面5折起，共計200櫃全面力拚比市場再低1折，500多件保暖外套平均1000~1500元；保暖冬被1折起。營業副理陳英玒看好在空污議題、低溫助攻下，家電業績可成長30%。\\n \\n民眾陳小姐表示今天以靴款為採買重點；從事行政的王小姐則主攻保養品、彩妝、香水，偏好在同一櫃買齊，折扣累積更划算，預算無上限；從事補教業的蒲先生同樣以購買保養品為主，保濕、眼霜、精華液等，大約花費1萬元。\\n \\n台北站前店今起至24日為期18天，首波強勢祭出超值禮券回饋，20日前滿2,000送200、全館滿5,000送500、大家電/法雅客滿萬送500、13日前加碼貴賓卡獨享大家電滿萬送仟。\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeKey(news, ['化妝品', '新光三越'])\n",
    "removeKey2(news, ['化妝品', '新光三越'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "全台最後一場週年慶今日由新光三越台北站前店壓軸登場\n",
      "化妝品推出20萬組明星特惠組\n",
      "保暖外套全面5折起\n",
      "吸引上千民眾排隊搶購\n",
      "館內一度塞車7分鐘\n",
      "周末又有一波強烈冷氣團來襲\n",
      "百貨業者看好禦寒商品\n",
      "抗空污家電持續成長\n",
      "預估首日來客數可達7萬人\n",
      "業績上看4.5億元\n",
      "成長1%\n",
      "\n",
      " \n",
      "台北站前店店長曹龍生表示\n",
      "今年化妝品表現最好\n",
      "各品牌口紅都熱賣\n",
      "年輕美眉也人手一支\n",
      "為彩妝引進許多新客\n",
      "帶動業績大幅成長\n",
      "另外近來全台深受空污影響\n",
      "讓空氣清淨機熱賣\n",
      "如LG空氣清淨機大缺貨\n",
      "家電新機種多優先在百貨開賣\n",
      "搭配滿千送百更划算\n",
      "\n",
      " \n",
      "冷氣團一波接著一波\n",
      "毛料外套\n",
      "羽絨被也相當受歡迎\n",
      "如羽毛工房限量100件\n",
      "開店不久即搶光\n",
      "新光三越台北站前店推出話題大衣全面5折起\n",
      "共計200櫃全面力拚比市場再低1折\n",
      "500多件保暖外套平均1000~1500元\n",
      "保暖冬被1折起\n",
      "營業副理陳英玒看好在空污議題\n",
      "低溫助攻下\n",
      "家電業績可成長30%\n",
      "\n",
      " \n",
      "民眾陳小姐表示今天以靴款為採買重點\n",
      "從事行政的王小姐則主攻保養品\n",
      "彩妝\n",
      "香水\n",
      "偏好在同一櫃買齊\n",
      "折扣累積更划算\n",
      "預算無上限\n",
      "從事補教業的蒲先生同樣以購買保養品為主\n",
      "保濕\n",
      "眼霜\n",
      "精華液等\n",
      "大約花費1萬元\n",
      "\n",
      " \n",
      "新光三越台北站前店今起至24日為期18天\n",
      "首波強勢祭出超值禮券回饋\n",
      "20日前化妝品滿2,000送200\n",
      "全館滿5,000送500\n",
      "大家電\n",
      "法雅客滿萬送500\n",
      "13日前加碼貴賓卡獨享大家電滿萬送仟\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for ele in re.split('，|。|、|；|/', news):\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "sentenceAry = []\n",
    "for ele in re.split('，|。|、|；|/', news):\n",
    "    sentenceAry.append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = '''\n",
    "行政院院會今通過《礦業法》修正草案，在礦場補做環評方面，採面積及產量分級管制，礦業用地面積大於2公頃、最近5年內年平均生產量5萬公噸以上的大型礦場，3年內補做環評；在此標準以下的小型礦場，則必須在5年內辦理環境影響調查、分析，並提出因應對策，若屆時未依法辦理補做環評，主管機關可處罰並要求限期改善，若屆期仍未改善者，最重可廢止其原核定的礦業用地。\n",
    " \n",
    "經濟部礦物局長徐景文今在院會後記者會表示，若修法通過，大型礦場3年內依《環境影響評估法》補做環評，共有6個水泥礦場，包括榮豐、台塑、潤泰2個、宜大、亞泥；小型礦場共有60個須在5年內依《環評法》辦理環境影響調查、分析。 \n",
    "\n",
    "經濟部礦業局官員在院會後記者會則表示，目前約有50件已申請礦展限的案件送到礦務局辦理展現，但立法院日前決議在明年3月前不得准駁展限。根據先前聽取學者專家意見，未來《礦業法》修法通過後，該50件已申請礦展限的案件將依據《中央標準法》的相關規定，針對法規採取「從優從新」方向辦理，因此須補辦環評、但不用經原住民族諮商同意。\n",
    " \n",
    "根據《礦業法》修正草案，「修正之條文施行後提出之礦業權展限申請時，其原核定礦業用地位於原住民族土地或部落及其周邊一定範圍內之公有土地，礦業權者應依《原住民族基本法》第21條(原住民族的諮商同意權)辦理。礦業局官員說，因此該50件是在修正之條文施行「前」提出，因此依法就無須經《原基本》第21條(原住民族的諮商同意權)辦理。\n",
    " \n",
    "據該草案規定，凡未曾實施環境影響評估者，在礦場補做環評方面，採面積及產量分級管制，礦業用地面積大於2公頃、最近5年內年平均生產量5萬公噸以上的大型礦場，3年內補做環評；在此標準以下的小型礦場，則必須在5年內辦理環境影響調查、分析。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n行政院院會今通過',\n",
       " '礦業法',\n",
       " '修正草案',\n",
       " '在礦場補做環評方面',\n",
       " '採面積及產量分級管制',\n",
       " '礦業用地面積大於2公頃',\n",
       " '最近5年內年平均生產量5萬公噸以上的大型礦場',\n",
       " '3年內補做環評',\n",
       " '在此標準以下的小型礦場',\n",
       " '則必須在5年內辦理環境影響調查',\n",
       " '分析',\n",
       " '並提出因應對策',\n",
       " '若屆時未依法辦理補做環評',\n",
       " '主管機關可處罰並要求限期改善',\n",
       " '若屆期仍未改善者',\n",
       " '最重可廢止其原核定的礦業用地',\n",
       " '\\n \\n經濟部礦物局長徐景文今在院會後記者會表示',\n",
       " '若修法通過',\n",
       " '大型礦場3年內依',\n",
       " '環境影響評估法',\n",
       " '補做環評',\n",
       " '共有6個水泥礦場',\n",
       " '包括榮豐',\n",
       " '台塑',\n",
       " '潤泰2個',\n",
       " '宜大',\n",
       " '亞泥',\n",
       " '小型礦場共有60個須在5年內依',\n",
       " '環評法',\n",
       " '辦理環境影響調查',\n",
       " '分析',\n",
       " ' \\n\\n經濟部礦業局官員在院會後記者會則表示',\n",
       " '目前約有50件已申請礦展限的案件送到礦務局辦理展現',\n",
       " '但立法院日前決議在明年3月前不得准駁展限',\n",
       " '根據先前聽取學者專家意見',\n",
       " '未來',\n",
       " '礦業法',\n",
       " '修法通過後',\n",
       " '該50件已申請礦展限的案件將依據',\n",
       " '中央標準法',\n",
       " '的相關規定',\n",
       " '針對法規採取',\n",
       " '從優從新',\n",
       " '方向辦理',\n",
       " '因此須補辦環評',\n",
       " '但不用經原住民族諮商同意',\n",
       " '\\n \\n根據',\n",
       " '礦業法',\n",
       " '修正草案',\n",
       " '',\n",
       " '修正之條文施行後提出之礦業權展限申請時',\n",
       " '其原核定礦業用地位於原住民族土地或部落及其周邊一定範圍內之公有土地',\n",
       " '礦業權者應依',\n",
       " '原住民族基本法',\n",
       " '第21條',\n",
       " '原住民族的諮商同意權',\n",
       " '辦理',\n",
       " '礦業局官員說',\n",
       " '因此該50件是在修正之條文施行',\n",
       " '前',\n",
       " '提出',\n",
       " '因此依法就無須經',\n",
       " '原基本',\n",
       " '第21條',\n",
       " '原住民族的諮商同意權',\n",
       " '辦理',\n",
       " '\\n \\n據該草案規定',\n",
       " '凡未曾實施環境影響評估者',\n",
       " '在礦場補做環評方面',\n",
       " '採面積及產量分級管制',\n",
       " '礦業用地面積大於2公頃',\n",
       " '最近5年內年平均生產量5萬公噸以上的大型礦場',\n",
       " '3年內補做環評',\n",
       " '在此標準以下的小型礦場',\n",
       " '則必須在5年內辦理環境影響調查',\n",
       " '分析',\n",
       " '\\n\\n']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sentenceAry = []\n",
    "for ele in re.split('，|。|、|；|/|》|《|\\)|「|\\(|」', news):\n",
    "    sentenceAry.append(ele)\n",
    "sentenceAry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['須在5年內', '辦理環境影', '理環境影響', '環境影響調', '境影響調查', '補做環評', '原住民族', '礦業用地', '大型礦場', '小型礦場', '諮商同意', '礦業法', '響調查', '年內', '辦理', '修正', '面積', '產量', '礦業', '展限', '院會', '通過', '草案', '礦場', '標準', '分析', '提出', '申請', '因此']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "keywords = []\n",
    "for term_length in range(5,1,-1):\n",
    "    ret = []\n",
    "    for sentence in sentenceAry:\n",
    "        s = removeKey(sentence, keywords)\n",
    "        ngram_words = ngram(s, term_length)\n",
    "        ret.extend(ngram_words)\n",
    "    c = Counter(ret)\n",
    "    for k, v in c.most_common():\n",
    "        m = re.match('[\\u4e00-\\u9fa5]+', k)\n",
    "        if v >= 3 and m:\n",
    "            keywords.append(k)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文字雲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = '''\n",
    "大陸冷氣團襲台，適逢水氣充足，玉山今日凌晨二度降下瑞雪！玉山北峰氣象站工作人員觀測到零時30分起飄落雪花，約莫持續到凌晨2時，降雪達1.5公分，上午目測累積雪量達到2公分，舉目望去一片雪白景象美不勝收，宣告今年冬季正式來臨。\n",
    "\n",
    "玉山國家公園管理處副處長林文和表示，今晨排雲山莊工作人員巡查玉山、勘查玉山主峰步道的雪況，發現從排雲山莊往上1公里步道開始有積雪現象，沿線步道積雪高度約1至2公分，由於步道濕滑，雖然排雲山莊目前積雪未達5公分，並未啟動雪季管制，但仍採柔性勸導，呼籲登山客須備齊頭盔、繩索、安全扣環、冰爪、冰斧等裝備，切勿貿然登頂。\n",
    "\n",
    "合歡山這頭接續前兩日追雪潮，上午卻因溫度不夠低，十時許武嶺氣溫2度，現場積雪融化八成，百餘名遊客收集殘雪堆小雪人過過癮。部分民眾在上山賞雪途中意外目睹廬山、清境農場方向大片秀麗雲海，紛紛停下腳步駐足拍照，直呼「美麗的意外！」\n",
    "\n",
    "陳姓民眾一早和妻子從桃園龜山駕車上山賞雪，見武嶺地區濃霧壟罩、殘雪消退，內心難掩失望，但在下山途中行經台14甲線28公里處，望見雲海美景立刻下車拍照，彌補缺憾。兩人緩步欣賞遠處山巒層層疊疊，雲朵如花團錦簇，讚嘆景色誘人，「有種小家碧玉的秀麗！」另有馬來西亞、新加坡等外國旅行團一行十餘遊客，儘管只見殘雪，仍大聲驚嘆，開心表示不虛此行，來台旅遊九天收穫豐碩。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "words_ary = []\n",
    "for word in jieba.cut(news):\n",
    "    words_ary.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words_ary = [word for word in jieba.cut(news)]\n",
    "c = Counter(words_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "玉山 4\n",
      "公分 4\n",
      "步道 4\n",
      "積雪 3\n",
      "凌晨 2\n",
      "工作 2\n",
      "上午 2\n",
      "表示 2\n",
      "排雲山莊 2\n",
      "公里 2\n",
      "上山 2\n",
      "賞雪 2\n",
      "途中 2\n",
      "意外 2\n",
      "拍照 2\n"
     ]
    }
   ],
   "source": [
    "for k, v in c.most_common(100):\n",
    "    if len(k) >= 2 and v >= 2:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將量化統計包裝成函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCount(news):\n",
    "    words_ary = [word for word in jieba.cut(news)]\n",
    "    c = Counter(words_ary)\n",
    "    for k, v in c.most_common(100):\n",
    "        if len(k) >= 2 and v >= 2:\n",
    "            print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = '''\n",
    "俄羅斯遭IOC(International Olympic Committee，國際奧林匹克委員會)禁止以國家名義參加明年平昌冬季奧運，通過藥檢該國運動員只能以獨立身分、「來自俄羅斯的奧林匹克運動員」參賽。俄羅斯總統普丁(Vladimir Putin)首次公開立場，表示不會阻止運動員獨立參賽，但不無反諷指出IOC只有對俄羅斯嚴格，也否認國家主導使用禁藥。\n",
    " \n",
    "普丁說：「我們不會採取任何阻擋運動員參賽措施。如果他們決定以獨立運動員身分參賽，我們當然還是會放行。我很關心這些選手，認識其中很多人，也把他們當朋友。他們一生都在為比賽不停奮鬥。」\n",
    " \n",
    "但話鋒一轉，普丁開酸：「不像其他國運動員，俄羅斯選手都會接受三重檢查。這意味什麼？所以我們不會設限，讓他們參與奧運變不可能。IOC之前指控我們以國家主導禁藥計畫的調查也沒有得到結論。懲罰應該是給有罪的人，如果從來沒有國家主導或資助的禁藥系統，為什麼運動員不能揮著俄羅斯國旗參賽？」\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "運動員 5\n",
      "參賽 5\n",
      "俄羅斯 4\n",
      "我們 4\n",
      "IOC 3\n",
      "普丁 3\n",
      "不會 3\n",
      "家主 3\n",
      "他們 3\n",
      "國運動員 2\n",
      "身分 2\n",
      "獨立 2\n",
      "禁藥 2\n",
      "如果 2\n",
      "選手 2\n",
      "什麼 2\n"
     ]
    }
   ],
   "source": [
    "wordCount(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 蘋果新聞最長出現字詞統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://tw.appledaily.com/new/realtime/1')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "news_ary = []\n",
    "for ele in soup.select('.rtddt a'):\n",
    "    res2 = requests.get(ele['href'])\n",
    "    soup2 = BeautifulSoup(res2.text, 'html.parser')\n",
    "    content = soup2.select_one('.ndArticle_margin p').text\n",
    "    news_ary.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for news in news_ary:\n",
    "    words.extend([ele for ele in jieba.cut(news)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "法官 40\n",
      "報導 36\n",
      "表示 30\n",
      "台灣 25\n",
      "警察 23\n",
      "公司 21\n",
      "益生菌 17\n",
      "和解 17\n",
      "因為 16\n",
      "社會 16\n",
      "球菌 16\n",
      "蘋果 16\n",
      "新聞 15\n",
      "部分 15\n",
      "台北 15\n",
      "因此 15\n",
      "中心 14\n",
      "12 14\n",
      "美國 14\n",
      "投資人 14\n",
      "要求 13\n",
      "今年 13\n",
      "腸道 13\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(words)\n",
    "for k, v in c.most_common(100):\n",
    "    if len(k) >= 2 and v >= 2:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, abb,abc = ['a'], ['a', 'b', 'b'], ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = [a,abb, abc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf('a', a, D)\n",
    "tf  = 1/1\n",
    "idf = sp.log(3/3)\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf('a', abb, D)\n",
    "tf = 1/3\n",
    "idf = sp.log(3/ 3)\n",
    "tf * idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27031007207210955"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf('b', abb, D)\n",
    "tf = 2/3\n",
    "idf = sp.log(3/2)\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf('a', abc , D)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13515503603605478"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf('b', abc, D)\n",
    "tf = 1/3\n",
    "idf = sp.log(3/2)\n",
    "tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36620409622270322"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf('c', abc, D)\n",
    "tf = 1/3\n",
    "idf = sp.log(3/1)\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(t, d, D):\n",
    "    tf  = d.count(t) / len(d)\n",
    "    idf = sp.log(len(D)/len([e for e in D if t in e ]))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27031007207210955"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf('b', abb, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13515503603605478"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf('b', abc, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36620409622270322"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf('c', abc, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 詞頻矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f4185e79cebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m ary = [\n\u001b[1;32m      3\u001b[0m \u001b[0;34m'富邦人壽首創保費1萬就能刷卡'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m'富邦人壽不老勇士Go來盃3對3籃賽南北開打'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m '富邦人壽不老勇士Go來盃開打']\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "ary = [\n",
    "'富邦人壽首創保費1萬就能刷卡',\n",
    "'富邦人壽不老勇士Go來盃3對3籃賽南北開打',\n",
    "'富邦人壽不老勇士Go來盃開打']\n",
    "\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "corpus = []\n",
    "for title in ary:\n",
    "    corpus.append(' '.join(jieba.cut(title)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', '不老', '人壽', '保費', '刷卡', '勇士', '南北', '富邦', '籃賽', '開打', '首創']\n",
      "[[0 0 1 1 1 0 0 1 0 0 1]\n",
      " [1 1 1 0 0 1 1 1 1 1 0]\n",
      " [1 1 1 0 0 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', '不老', '人壽', '保費', '刷卡', '勇士', '南北', '富邦', '籃賽', '開打', '首創']\n",
      "[[ 0.          0.          0.30714405  0.52004008  0.52004008  0.          0.\n",
      "   0.30714405  0.          0.          0.52004008]\n",
      " [ 0.32628714  0.32628714  0.25339107  0.          0.          0.32628714\n",
      "   0.42902838  0.25339107  0.42902838  0.42902838  0.        ]\n",
      " [ 0.48759135  0.48759135  0.37865818  0.          0.          0.48759135\n",
      "   0.          0.37865818  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
